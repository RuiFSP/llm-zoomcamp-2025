{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e38a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the minsearch.py file from the specified GitHub repository\n",
    "#!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/refs/heads/main/minsearch/minsearch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00a33c",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Systems\n",
    "\n",
    "This notebook demonstrates the implementation of Retrieval-Augmented Generation (RAG) systems using two different search backends:\n",
    "\n",
    "1. **MinSearch**: A lightweight, in-memory search implementation\n",
    "2. **Elasticsearch**: A powerful, scalable search engine\n",
    "\n",
    "We'll build a complete RAG pipeline that:\n",
    "- Indexes course FAQ documents\n",
    "- Retrieves relevant documents for user questions\n",
    "- Augments prompts with retrieved context\n",
    "- Generates accurate answers using OpenAI models\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Setup & Data Loading**: Import libraries and load FAQ documents\n",
    "2. **MinSearch Implementation**: Create and use a simple vector search engine\n",
    "3. **OpenAI Integration**: Connect to OpenAI for LLM-based responses\n",
    "4. **Basic RAG Pipeline**: Combine search and language model components\n",
    "5. **Elasticsearch Integration**: Add a more powerful search backend\n",
    "6. **Comparison & Evaluation**: Compare different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f426f65-7904-4231-92a5-e899547d90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install minsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3da3ec",
   "metadata": {},
   "source": [
    "# Using the minsearch index to search for a specific question\n",
    "\n",
    "- this is a costum class created that uses TfidfVectorizer and cosine_simillarity\n",
    "\n",
    "# Part 1: MinSearch Implementation\n",
    "\n",
    "MinSearch is a lightweight search engine that uses:\n",
    "- **TF-IDF Vectorization**: Converts text into numerical vectors based on term frequency and inverse document frequency\n",
    "- **Cosine Similarity**: Measures similarity between query and documents\n",
    "- **Field Boosting**: Allows prioritizing certain fields (e.g., question vs text)\n",
    "\n",
    "This implementation is ideal for small to medium document collections that fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63b0fd25-41a2-48ad-b9bf-3f1265308bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for MinSearch implementation\n",
    "import minsearch  # Simple vector search engine\n",
    "import json       # For loading document data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73710834",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "We'll load FAQ documents from a JSON file and prepare them for indexing. Each document contains:\n",
    "- **question**: The user question\n",
    "- **text**: The answer text\n",
    "- **section**: The section/category the question belongs to\n",
    "- **course**: The course the question is related to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bdaf6ce-2540-494f-989c-5b94b1b6626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42b9b1f9-3c90-42b0-beb4-cb419f9cdcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57de60e5-b96c-499c-a7cf-0f30fc33b324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c499838b-73b3-44be-8ba6-f46d3693aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"], # fields to search in\n",
    "    keyword_fields=[\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8d8ea88-7412-49c1-8a8e-44d0d0862a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'the course has already started, can I still enroll?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce7d0d18-5c07-4010-9f90-bbd021f110c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f60a4482350>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents) # build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa755a08-b98d-4e92-8994-04e6108499d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from: ../../.env\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for OpenAI integration\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (contains API keys)\n",
    "env_path = Path('../..') / '.env'\n",
    "\n",
    "print(f\"Loading environment variables from: {env_path}\")\n",
    "\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "else:\n",
    "    print(\"⚠️ Warning: .env file not found, make sure to set OPENAI_API_KEY manually\")\n",
    "\n",
    "# Access the API key\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562606a",
   "metadata": {},
   "source": [
    "# Part 2: OpenAI LLM Integration\n",
    "\n",
    "We'll integrate OpenAI's models to generate responses based on retrieved documents. This involves:\n",
    "1. Setting up the OpenAI client with API keys\n",
    "2. Defining a function to create prompts with retrieved context\n",
    "3. Building a function to generate answers using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef8e9cdc-dfd4-4e54-a332-4b9bde4e6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7da9664-ecb3-4d89-87da-9b2b942444d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whether you can still enroll in a course that has already started depends on the institution or program's policies. Many schools and universities have specific deadlines for enrollment, while some may allow late registration under certain circumstances. I recommend checking the course's official website or contacting the admissions office or course instructor directly for the most accurate information regarding late enrollment options.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[{\"role\": \"user\", \"content\": q}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b21237c3-80e9-429c-a089-d45428087046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced search function for MinSearch implementation\n",
    "def search(query):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents using MinSearch vector search.\n",
    "    \n",
    "    This function:\n",
    "    1. Performs a semantic search based on the query\n",
    "    2. Applies field boosting to prioritize question matches\n",
    "    3. Filters results to the target course\n",
    "    \n",
    "    Args:\n",
    "        query: User's natural language question\n",
    "        \n",
    "    Returns:\n",
    "        list: Ranked list of relevant documents\n",
    "    \"\"\"\n",
    "    # Configure boosting weights for different fields\n",
    "    boost = {\n",
    "        'question': 3.0,  # Questions are highly relevant (3x weight)\n",
    "        'section': 0.5    # Sections are less relevant (0.5x weight)\n",
    "    }\n",
    "\n",
    "    # Perform the search with filtering and boosting\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},  # Filter by course\n",
    "        boost_dict=boost,                                     # Apply field boosting\n",
    "        num_results=5                                         # Return top 5 matches\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cc5784e-6515-42e5-be62-8fb915df1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the prompt for the LLM\n",
    "# based on the search results\n",
    "# and the original query\n",
    "# the prompt will be used to answer the question\n",
    "\n",
    "# Enhanced prompt builder function with detailed comments\n",
    "def build_prompt(query, search_results):\n",
    "    \"\"\"\n",
    "    Build an LLM prompt that includes retrieved context documents.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses a template to structure the prompt\n",
    "    2. Formats each retrieved document into the context\n",
    "    3. Inserts the original query and formatted context into template\n",
    "    \n",
    "    Args:\n",
    "        query: User's original question\n",
    "        search_results: List of retrieved documents\n",
    "        \n",
    "    Returns:\n",
    "        string: Formatted prompt ready for the LLM\n",
    "    \"\"\"\n",
    "    # Template that structures the prompt with placeholders\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    # Format each retrieved document into context section\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    # Fill in template with query and formatted context\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97d35dec-c25f-472d-b961-20d5c30902ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LLM function for generating responses\n",
    "def llm(prompt):\n",
    "    \"\"\"\n",
    "    Generate a response using OpenAI's LLM based on the provided prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The full prompt including query and context\n",
    "        \n",
    "    Returns:\n",
    "        string: Generated answer from the LLM\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',  # Using OpenAI's most capable model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2  # Lower temperature for more factual responses\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8602f40b-ad3b-49c9-b3cc-051a79c888bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test query\n",
    "query = 'how do I run kafka?'\n",
    "\n",
    "# Complete RAG pipeline implementation\n",
    "def rag(query):\n",
    "    \"\"\"\n",
    "    Implements the full Retrieval-Augmented Generation pipeline.\n",
    "    \n",
    "    The pipeline consists of three main steps:\n",
    "    1. RETRIEVAL: Find relevant documents using vector search\n",
    "    2. AUGMENTATION: Build a prompt that includes retrieved context\n",
    "    3. GENERATION: Generate an answer using the LLM with context\n",
    "    \n",
    "    Args:\n",
    "        query: User's natural language question\n",
    "        \n",
    "    Returns:\n",
    "        string: Generated answer based on retrieved context\n",
    "    \"\"\"\n",
    "    # Step 1: RETRIEVAL - Get relevant documents using MinSearch\n",
    "    search_results = search(query)\n",
    "    \n",
    "    # Step 2: AUGMENTATION - Build prompt with retrieved context\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    \n",
    "    # Step 3: GENERATION - Generate answer using LLM\n",
    "    answer = llm(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc8625",
   "metadata": {},
   "source": [
    "# Part 3: Building the Basic RAG Pipeline\n",
    "\n",
    "Now we'll build the complete RAG pipeline that combines:\n",
    "\n",
    "1. **Retrieval**: Using MinSearch to find relevant documents\n",
    "2. **Augmentation**: Creating a prompt with retrieved context\n",
    "3. **Generation**: Using OpenAI to generate answers\n",
    "\n",
    "This pattern ensures that the LLM's responses are grounded in our specific documents rather than its general knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd4497b-c5d5-4258-b950-6b35d1af4ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with query: 'how do I run kafka?'\n",
      "--------------------------------------------------------------------------------\n",
      "To run Kafka in the terminal, you can execute the following command in the project directory:\n",
      "\n",
      "```bash\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "```\n",
      "\n",
      "Replace `<jar_name>` with the actual name of your JAR file.\n",
      "To run Kafka in the terminal, you can execute the following command in the project directory:\n",
      "\n",
      "```bash\n",
      "java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n",
      "```\n",
      "\n",
      "Replace `<jar_name>` with the actual name of your JAR file.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG pipeline with our first query about Kafka\n",
    "print(\"Testing with query: 'how do I run kafka?'\")\n",
    "print(\"-\" * 80)\n",
    "response = rag(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33796a",
   "metadata": {},
   "source": [
    "# Testing the Basic RAG Implementation\n",
    "\n",
    "Let's test our RAG pipeline with different queries to see how well it retrieves relevant information and generates answers. We'll try:\n",
    "\n",
    "1. A technical question about Kafka\n",
    "2. A course enrollment question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "385b012f-4905-422d-8d7c-3d542dfe5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with query: 'the course has already started, can I still enroll?'\n",
      "--------------------------------------------------------------------------------\n",
      "Yes, you can still enroll in the course even after it has started. You are eligible to submit the homework assignments. However, be mindful of the deadlines for submitting the final projects and try not to leave everything until the last minute.\n",
      "Yes, you can still enroll in the course even after it has started. You are eligible to submit the homework assignments. However, be mindful of the deadlines for submitting the final projects and try not to leave everything until the last minute.\n"
     ]
    }
   ],
   "source": [
    "rag('the course has already started, can I still enroll?')\n",
    "# Test with another query\n",
    "print(\"Testing with query: 'the course has already started, can I still enroll?'\")\n",
    "print(\"-\" * 80)\n",
    "response = rag('the course has already started, can I still enroll?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "888e3bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top search results for: 'the course has already started, can I still enroll?'\n",
      "--------------------------------------------------------------------------------\n",
      "Result 1:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I still join the course after the start date?\n",
      "- Answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the fin...\n",
      "\n",
      "Result 2:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I follow the course after it finishes?\n",
      "- Answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue ...\n",
      "\n",
      "Result 3:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - When will the course start?\n",
      "- Answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. T...\n",
      "\n",
      "Result 4:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - What can I do before the course starts?\n",
      "- Answer: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaco...\n",
      "\n",
      "Result 5:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I get support if I take the course in the self-paced mode?\n",
      "- Answer: Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, ch...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic function to examine search results\n",
    "def inspect_search_results(query):\n",
    "    \"\"\"\n",
    "    Show the actual search results for a query to diagnose relevance\n",
    "    \n",
    "    Args:\n",
    "        query: The user query to search for\n",
    "        \n",
    "    Returns:\n",
    "        None (prints results)\n",
    "    \"\"\"\n",
    "    results = search(query)\n",
    "    print(f\"Top search results for: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"Result {i+1}:\")\n",
    "        print(f\"- Section: {doc['section']}\")\n",
    "        print(f\"- Question: {doc['question']}\")\n",
    "        print(f\"- Answer: {doc['text'][:150]}...\" if len(doc['text']) > 150 else doc['text'])\n",
    "        print()\n",
    "\n",
    "# Inspect search results for our test query\n",
    "inspect_search_results('the course has already started, can I still enroll?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3e04fb3-b7f7-4e53-8de9-a1c6cde3f038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fd651",
   "metadata": {},
   "source": [
    "# Using Elasticsearch for RAG\n",
    "\n",
    "Elasticsearch is a powerful search engine that provides more advanced search capabilities than our simple MinSearch implementation. It offers:\n",
    "\n",
    "- Full-text search with ranking\n",
    "- Relevance scoring\n",
    "- Filtering capabilities\n",
    "- Scalability for large document collections\n",
    "\n",
    "In this section, we'll set up Elasticsearch in Docker and connect to it from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e5a8a",
   "metadata": {},
   "source": [
    "# Setup: Elasticsearch Server and Python Client\n",
    "\n",
    "## Installation and Configuration\n",
    "\n",
    "For Elasticsearch to work properly, we need to ensure version compatibility between the server and client.\n",
    "\n",
    "```bash\n",
    "# Install Elasticsearch Python client version 7.17.0\n",
    "# We use this specific version because:\n",
    "# 1. It's compatible with Elasticsearch 8.x servers\n",
    "# 2. It doesn't attempt to use version 9 API headers that newer clients use\n",
    "uv pip uninstall -y elasticsearch\n",
    "uv pip install elasticsearch==7.17.0\n",
    "\n",
    "# Run Elasticsearch 8.x in Docker\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.12.2\n",
    "```\n",
    "\n",
    "## Version Compatibility Issues Explained\n",
    "\n",
    "When working with Elasticsearch, the client and server versions need to be compatible:\n",
    "\n",
    "1. **API Versioning**: Newer ES Python clients (8.x) send requests with version 9 API compatibility headers\n",
    "2. **Server Limitations**: ES servers only accept API requests with the same major version or one version lower\n",
    "3. **Header Conflicts**: The error \"Accept version must be either version 8 or 7, but found 9\" occurs when these versions mismatch\n",
    "\n",
    "**Our Solution**: Use the ES 7.17.0 client with ES 8.x server for compatibility, or override the default headers to use non-versioned API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f929b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.transport import Transport\n",
    "\n",
    "# === VERSION COMPATIBILITY FIX ===\n",
    "# Override default headers to prevent version compatibility issues\n",
    "# - By default, newer ES clients send headers with \"compatible-with=9\"\n",
    "# - ES 8.x servers reject these headers, causing BadRequestError (400)\n",
    "# - We explicitly set headers to use standard JSON without version info\n",
    "Transport._DEFAULT_REQUEST_HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Create the Elasticsearch client with connection settings\n",
    "es_client = Elasticsearch(\n",
    "    'http://localhost:9200',  # Default Elasticsearch endpoint\n",
    "    request_timeout=30,       # Increased timeout for slower operations\n",
    "    verify_certs=False,       # Disable SSL verification for local development\n",
    "    api_key=None,             # No authentication for our local instance\n",
    "    basic_auth=None,          # No basic auth credentials\n",
    "    ca_certs=None             # No CA certificates for SSL verification\n",
    ")\n",
    "\n",
    "# Note: For production environments, you would enable security features\n",
    "# and proper certificate validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4379b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Elasticsearch\n",
      "Elasticsearch version: 8.12.2\n",
      "Cluster name: docker-cluster\n"
     ]
    }
   ],
   "source": [
    "# Test the Elasticsearch connection\n",
    "# - This will verify our client configuration is correct\n",
    "# - It returns information about the Elasticsearch server\n",
    "try:\n",
    "    info = es_client.info()\n",
    "    print(\"✅ Successfully connected to Elasticsearch\")\n",
    "    print(f\"Elasticsearch version: {info['version']['number']}\")\n",
    "    print(f\"Cluster name: {info['cluster_name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {e}\")\n",
    "    print(\"Check that Elasticsearch is running and client compatibility settings are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc662ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Index course-questions already exists\n"
     ]
    }
   ],
   "source": [
    "# Create Elasticsearch index with proper error handling\n",
    "try:\n",
    "    # Define the index schema (mappings and settings)\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,       # Use 1 shard for simplicity (dev environment)\n",
    "            \"number_of_replicas\": 0      # No replicas needed for local development\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                # Field mappings determine how Elasticsearch indexes each field:\n",
    "                \"text\": {\"type\": \"text\"},        # Full-text search for answer content\n",
    "                \"section\": {\"type\": \"text\"},     # Full-text search for section names\n",
    "                \"question\": {\"type\": \"text\"},    # Full-text search for questions\n",
    "                \"course\": {\"type\": \"keyword\"}    # Exact match filtering for course names\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Index name for our course questions\n",
    "    index_name = \"course-questions\"\n",
    "    \n",
    "    # Check if the index already exists to avoid duplicate creation\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name, body=index_settings)\n",
    "        print(f\"✅ Created new Elasticsearch index: {index_name}\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Index {index_name} already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating Elasticsearch index: {e}\")\n",
    "    print(\"Will continue with MinSearch instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f778c93-a5b6-4634-b42e-0c25083a2512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe3c97-916d-42c0-bd7b-4f42d9056409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Indexing documents into Elasticsearch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e99f6d0819418e8125e42073601043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully indexed 948 documents\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Index documents in Elasticsearch with progress bar\n",
    "try:\n",
    "    print(\"🔍 Indexing documents into Elasticsearch...\")\n",
    "    \n",
    "    # Use tqdm for a nice progress bar to track indexing\n",
    "    for doc in tqdm(documents):\n",
    "        # The index() method:\n",
    "        # - Adds each document to the specified index\n",
    "        # - Automatically generates an ID if not provided\n",
    "        # - Sets document field values according to our mapping\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    \n",
    "    print(f\"✅ Successfully indexed {len(documents)} documents\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error indexing documents: {e}\")\n",
    "    print(\"Falling back to MinSearch functionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1bc1244-b8dc-4228-8171-c0507004db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just disovered the course. Can I still join it?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c72e000-910b-4fb5-aa88-2561e7bc39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced search function using Elasticsearch\n",
    "def elastic_search(query):\n",
    "    try:\n",
    "        # Construct an Elasticsearch query with:\n",
    "        # - Multi-match for searching across multiple fields with different weights\n",
    "        # - Boolean filtering to restrict results to specific courses\n",
    "        search_query = {\n",
    "            \"size\": 5,  # Limit to 5 results\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": {\n",
    "                        # Multi-match searches across multiple fields\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            # Search in three fields with boosted relevance for questions\n",
    "                            \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                            \"type\": \"best_fields\"  # Prioritize fields with highest match\n",
    "                        }\n",
    "                    },\n",
    "                    # Filter to only include specific course documents\n",
    "                    \"filter\": {\n",
    "                        \"term\": {\n",
    "                            \"course\": \"data-engineering-zoomcamp\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute search and get results\n",
    "        response = es_client.search(index=index_name, body=search_query)\n",
    "        \n",
    "        # Extract just the source documents from search hits\n",
    "        result_docs = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            result_docs.append(hit['_source'])\n",
    "        \n",
    "        return result_docs\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error searching with Elasticsearch: {e}\")\n",
    "        print(\"Falling back to MinSearch...\")\n",
    "        # Graceful fallback to our simple search if ES fails\n",
    "        return search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81abecbc-eb6b-428f-ab7d-7e21f58b64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated RAG function with Elasticsearch integration\n",
    "def rag(query):\n",
    "    \"\"\"\n",
    "    Enhanced Retrieval-Augmented Generation (RAG) pipeline using Elasticsearch.\n",
    "    \n",
    "    This version replaces the MinSearch retrieval component with Elasticsearch for:\n",
    "    - Better relevance ranking\n",
    "    - Support for advanced query features\n",
    "    - Scalability to larger document collections\n",
    "    - Filtering capabilities\n",
    "    \n",
    "    The pipeline flow remains the same:\n",
    "    1. RETRIEVE: Find relevant documents using Elasticsearch\n",
    "    2. AUGMENT: Create prompt with retrieved context\n",
    "    3. GENERATE: Get LLM answer based on context\n",
    "    \n",
    "    Args:\n",
    "        query: User's natural language question\n",
    "        \n",
    "    Returns:\n",
    "        string: Generated answer based on retrieved context\n",
    "    \"\"\"\n",
    "    # Step 1: RETRIEVE - Get relevant documents using Elasticsearch\n",
    "    # The elastic_search function has a fallback to MinSearch if ES fails\n",
    "    search_results = elastic_search(query)\n",
    "    \n",
    "    # Step 2: AUGMENT - Build prompt with retrieved context\n",
    "    # Same prompt building logic works with either search backend\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    \n",
    "    # Step 3: GENERATE - Get LLM answer based on context\n",
    "    answer = llm(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac152477",
   "metadata": {},
   "source": [
    "# MinSearch vs. Elasticsearch: Key Differences\n",
    "\n",
    "Now that we've implemented both MinSearch and Elasticsearch for our RAG system, let's understand their key differences:\n",
    "\n",
    "| Feature | MinSearch | Elasticsearch |\n",
    "|---------|-----------|---------------|\n",
    "| **Complexity** | Simple, in-memory | Full-featured search engine |\n",
    "| **Scalability** | Limited to RAM | Horizontally scalable |\n",
    "| **Search Features** | Basic TF-IDF + cosine similarity | Advanced queries, filters, analyzers |\n",
    "| **Setup** | Simple Python import | Requires server infrastructure |\n",
    "| **Speed** | Fast for small datasets | Optimized for large-scale search |\n",
    "| **Deployment** | Single process | Distributed architecture |\n",
    "\n",
    "For our small FAQ dataset, both approaches work well. As your document collection grows larger or more complex queries are needed, Elasticsearch becomes increasingly valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ea9315a-a619-4066-9e90-8c260f2c8450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74626/2487162615.py:31: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=index_name, body=search_query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still join the course even after the start date. You are eligible to submit the homeworks even if you haven't registered. However, keep in mind that there will be deadlines for turning in the final projects, so it's important not to leave everything until the last minute.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f58ea3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARING SEARCH BACKENDS FOR: 'I just discovered the course. Can I still join it?'\n",
      "\n",
      "=== MINSEARCH RESULTS ===\n",
      "Top search results for: 'I just disovered the course. Can I still join it?'\n",
      "--------------------------------------------------------------------------------\n",
      "Result 1:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I still join the course after the start date?\n",
      "- Answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the fin...\n",
      "\n",
      "Result 2:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I follow the course after it finishes?\n",
      "- Answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue ...\n",
      "\n",
      "Result 3:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - When will the course start?\n",
      "- Answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. T...\n",
      "\n",
      "Result 4:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - What can I do before the course starts?\n",
      "- Answer: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaco...\n",
      "\n",
      "Result 5:\n",
      "- Section: General course-related questions\n",
      "- Question: How can we contribute to the course?\n",
      "Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve the text or the structure of the repository.\n",
      "\n",
      "\n",
      "=== ELASTICSEARCH RESULTS ===\n",
      "Top Elasticsearch results for: 'I just disovered the course. Can I still join it?'\n",
      "--------------------------------------------------------------------------------\n",
      "Result 1:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I still join the course after the start date?\n",
      "- Answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the fin...\n",
      "\n",
      "Result 2:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I still join the course after the start date?\n",
      "- Answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the fin...\n",
      "\n",
      "Result 3:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I follow the course after it finishes?\n",
      "- Answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue ...\n",
      "\n",
      "Result 4:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - Can I follow the course after it finishes?\n",
      "- Answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue ...\n",
      "\n",
      "Result 5:\n",
      "- Section: General course-related questions\n",
      "- Question: Course - What can I do before the course starts?\n",
      "- Answer: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaco...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74626/2487162615.py:31: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=index_name, body=search_query)\n"
     ]
    }
   ],
   "source": [
    "# Inspect Elasticsearch search results\n",
    "def inspect_elastic_search_results(query):\n",
    "    \"\"\"\n",
    "    Show the actual Elasticsearch search results for a query to diagnose relevance\n",
    "    \n",
    "    Args:\n",
    "        query: The user query to search for\n",
    "        \n",
    "    Returns:\n",
    "        None (prints results)\n",
    "    \"\"\"\n",
    "    results = elastic_search(query)\n",
    "    print(f\"Top Elasticsearch results for: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"Result {i+1}:\")\n",
    "        print(f\"- Section: {doc['section']}\")\n",
    "        print(f\"- Question: {doc['question']}\")\n",
    "        print(f\"- Answer: {doc['text'][:150]}...\" if len(doc['text']) > 150 else doc['text'])\n",
    "        print()\n",
    "\n",
    "# Compare search results between MinSearch and Elasticsearch\n",
    "print(\"COMPARING SEARCH BACKENDS FOR: 'I just discovered the course. Can I still join it?'\")\n",
    "print(\"\\n=== MINSEARCH RESULTS ===\")\n",
    "inspect_search_results(query)\n",
    "print(\"\\n=== ELASTICSEARCH RESULTS ===\")\n",
    "inspect_elastic_search_results(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b41ba4",
   "metadata": {},
   "source": [
    "# Conclusion: RAG System with Elasticsearch\n",
    "\n",
    "We've successfully built a complete Retrieval-Augmented Generation (RAG) system that:\n",
    "\n",
    "1. **Retrieves** relevant documents from an Elasticsearch index based on user queries\n",
    "2. **Augments** LLM prompts with the retrieved context information\n",
    "3. **Generates** accurate answers using the provided context\n",
    "\n",
    "This architecture offers several advantages:\n",
    "- 📚 **Knowledge Grounding**: Answers are based on specific documents rather than general knowledge\n",
    "- 🔍 **Source Attribution**: We know exactly which documents inform each answer\n",
    "- 🧠 **Domain Specificity**: The system knows about our specific course content\n",
    "- ⚡ **Efficient Resource Use**: Smaller context windows needed compared to embedding all documents\n",
    "\n",
    "The Elasticsearch integration enables more sophisticated retrieval than our basic MinSearch implementation, providing better search relevance, filtering capabilities, and potential for scaling to much larger document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d472f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING RAG SYSTEM WITH QUERY: 'What prerequisites do I need for the course?'\n",
      "================================================================================\n",
      "STEP 1: SEARCH RESULTS\n",
      "Result 1: Course - What are the prerequisites for this course?\n",
      "Result 2: Course - What are the prerequisites for this course?\n",
      "Result 3: Course - What can I do before the course starts?\n",
      "\n",
      "STEP 2: PROMPT CONSTRUCTION\n",
      "Prompt length: 2391 characters\n",
      "Prompt preview:\n",
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "QUESTION: What prerequisites do I need for the course?\n",
      "\n",
      "CONTEXT: \n",
      "section: General course-related questions\n",
      "question: Course - What ar...\n",
      "\n",
      "STEP 3: GENERATED ANSWER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74626/2487162615.py:31: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client.search(index=index_name, body=search_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prerequisites for the course can be found on the GitHub page of DataTalksClub under the data-engineering-zoomcamp section, specifically at the #prerequisites link.\n"
     ]
    }
   ],
   "source": [
    "# Final test of the complete Elasticsearch-powered RAG system\n",
    "def test_rag_system(query):\n",
    "    \"\"\"Run a comprehensive test of the RAG system with detailed output\"\"\"\n",
    "    print(f\"TESTING RAG SYSTEM WITH QUERY: '{query}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Show search results\n",
    "    print(\"STEP 1: SEARCH RESULTS\")\n",
    "    results = elastic_search(query)\n",
    "    for i, doc in enumerate(results[:3]): # Show top 3 results\n",
    "        print(f\"Result {i+1}: {doc['question']}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Show prompt construction\n",
    "    print(\"STEP 2: PROMPT CONSTRUCTION\")\n",
    "    prompt = build_prompt(query, results)\n",
    "    print(f\"Prompt length: {len(prompt)} characters\")\n",
    "    print(\"Prompt preview:\")\n",
    "    print(prompt[:300] + \"...\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Show generated answer\n",
    "    print(\"STEP 3: GENERATED ANSWER\")\n",
    "    answer = llm(prompt)\n",
    "    print(answer)\n",
    "\n",
    "# Test with a new question\n",
    "test_rag_system(\"What prerequisites do I need for the course?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
