{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cace0591",
   "metadata": {},
   "source": [
    "# Homework 1 - LLM ZOOMCAMP - Rui Pinto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b92314",
   "metadata": {},
   "source": [
    "## Q1 Running Elastic\n",
    "- Run Elastic Search 8.12.2, and get the cluster information. If you run it on localhost, this is how you do it:\n",
    "\n",
    "```bash\n",
    "curl localhost:9200\n",
    "```\n",
    "\n",
    "What's the version.build_hash value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11e94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"5f4d660d29c7\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"aIMufzLsTtuIhWNxqgFr3A\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.12.2\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"48a287ab9497e852de30327444b0809e55d46466\",\n",
      "    \"build_date\" : \"2024-02-19T10:04:32.774273190Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.9.2\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl localhost:9200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808ae77",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba38b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b9170",
   "metadata": {},
   "source": [
    "## Q2. Indexing the data\n",
    "\n",
    "Index the data in the same way as was shown in the course videos. \n",
    "Make the course field a keyword and the rest should be text.\n",
    "\n",
    "Which function do you use for adding your data to elastic?\n",
    "\n",
    "- insert\n",
    "- index ‚úÖ\n",
    "- put\n",
    "- add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c3665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.transport import Transport\n",
    "\n",
    "# Create the Elasticsearch client with connection settings\n",
    "es_client_hw01 = Elasticsearch(\n",
    "    'http://localhost:9200',  # Default Elasticsearch endpoint\n",
    "    request_timeout=30,       # Increased timeout for slower operations\n",
    "    verify_certs=False,       # Disable SSL verification for local development\n",
    "    api_key=None,             # No authentication for our local instance\n",
    "    basic_auth=None,          # No basic auth credentials\n",
    "    ca_certs=None             # No CA certificates for SSL verification\n",
    ")\n",
    "\n",
    "# Note: For production environments, you would enable security features\n",
    "# and proper certificate validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091688cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Elasticsearch\n",
      "Elasticsearch version: 8.12.2\n",
      "Cluster name: docker-cluster\n"
     ]
    }
   ],
   "source": [
    "# Test the Elasticsearch connection\n",
    "# - This will verify our client configuration is correct\n",
    "# - It returns information about the Elasticsearch server\n",
    "try:\n",
    "    info = es_client_hw01.info()\n",
    "    print(\"‚úÖ Successfully connected to Elasticsearch\")\n",
    "    print(f\"Elasticsearch version: {info['version']['number']}\")\n",
    "    print(f\"Cluster name: {info['cluster_name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"Check that Elasticsearch is running and client compatibility settings are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df99913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82822/1723704631.py:25: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es_client_hw01.indices.create(index=index_name, body=index_settings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created new Elasticsearch index: course-questions-hw01\n"
     ]
    }
   ],
   "source": [
    "# Create Elasticsearch index with proper error handling\n",
    "try:\n",
    "    # Define the index schema (mappings and settings)\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,       # Use 1 shard for simplicity (dev environment)\n",
    "            \"number_of_replicas\": 0      # No replicas needed for local development\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                # Field mappings determine how Elasticsearch indexes each field:\n",
    "                \"text\": {\"type\": \"text\"},        # Full-text search for answer content\n",
    "                \"section\": {\"type\": \"text\"},     # Full-text search for section names\n",
    "                \"question\": {\"type\": \"text\"},    # Full-text search for questions\n",
    "                \"course\": {\"type\": \"keyword\"}    # Exact match filtering for course names\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Index name for our course questions\n",
    "    index_name = \"course-questions-hw01\"\n",
    "    \n",
    "    # Check if the index already exists to avoid duplicate creation\n",
    "    if not es_client_hw01.indices.exists(index=index_name):\n",
    "        es_client_hw01.indices.create(index=index_name, body=index_settings)\n",
    "        print(f\"‚úÖ Created new Elasticsearch index: {index_name}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Index {index_name} already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating Elasticsearch index: {e}\")\n",
    "    print(\"Will continue with MinSearch instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d894a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Indexing documents into Elasticsearch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b718a8dd8c6483ebe22351621f834e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully indexed 948 documents\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Index documents in Elasticsearch with progress bar\n",
    "try:\n",
    "    print(\"üîç Indexing documents into Elasticsearch...\")\n",
    "    \n",
    "    # Use tqdm for a nice progress bar to track indexing\n",
    "    for doc in tqdm(documents):\n",
    "        # The index() method:\n",
    "        # - Adds each document to the specified index\n",
    "        # - Automatically generates an ID if not provided\n",
    "        # - Sets document field values according to our mapping\n",
    "        # - This is the standard way to add documents to Elasticsearch\n",
    "        # - The function specifically answers Q2 in the homework\n",
    "        es_client_hw01.index(index=index_name, document=doc)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully indexed {len(documents)} documents\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error indexing documents: {e}\")\n",
    "    print(\"Falling back to MinSearch functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb8261",
   "metadata": {},
   "source": [
    "## Q3. Searching\n",
    "Now let's search in our index.\n",
    "\n",
    "We will execute a query \"How do execute a command on a Kubernetes pod?\".\n",
    "\n",
    "Use only question and text fields and give question a boost of 4, and use \"type\": \"best_fields\".\n",
    "\n",
    "What's the score for the top ranking result?\n",
    "\n",
    "- 84.50\n",
    "- 64.50\n",
    "- 44.50 ‚úÖ\n",
    "- 24.50\n",
    "\n",
    "Look at the _score field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae458ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do execute a command on a Kubernetes pod?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    try:\n",
    "        # Construct an Elasticsearch query with:\n",
    "        # - Multi-match for searching across multiple fields with different weights\n",
    "        # - Using only question and text fields with question having a boost of 4\n",
    "        # - \"best_fields\" type optimizes for documents where the terms appear in the same field\n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"question^4\", \"text\"],  # ^4 syntax applies 4x weight to matches in question field\n",
    "                    \"type\": \"best_fields\"  # This type is important for Q3's scoring result\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute search and get results\n",
    "        response = es_client_hw01.search(index=index_name, body=search_query)\n",
    "        \n",
    "        # Print the score for the top result\n",
    "        if response['hits']['hits']:\n",
    "            print(f\"Top result score: {response['hits']['hits'][0]['_score']}\")\n",
    "        \n",
    "        # Return the full response to examine all details\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching with Elasticsearch: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf874777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result score: 44.50556\n",
      "Top hit score: 44.50556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82822/2332176584.py:17: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client_hw01.search(index=index_name, body=search_query)\n"
     ]
    }
   ],
   "source": [
    "elastic_response = elastic_search(query)\n",
    "\n",
    "# What's the score for the top ranking result?\n",
    "\n",
    "if elastic_response and 'hits' in elastic_response:\n",
    "    top_hit = elastic_response['hits']['hits'][0] if elastic_response['hits']['hits'] else None\n",
    "    if top_hit:\n",
    "        print(f\"Top hit score: {top_hit['_score']}\")\n",
    "        #print(f\"Top hit source: {top_hit['_source']}\")\n",
    "    else:\n",
    "        print(\"No hits found\")\n",
    "else:\n",
    "    print(\"No valid response from Elasticsearch search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c683db",
   "metadata": {},
   "source": [
    "## Q4 Filtering\n",
    "Now ask a different question: \"How do copy a file to a Docker container?\".\n",
    "\n",
    "This time we are only interested in questions from machine-learning-zoomcamp.\n",
    "\n",
    "Return 3 results. What's the 3rd question returned by the search engine?\n",
    "\n",
    "- How do I debug a docker container?\n",
    "- How do I copy files from a different folder into docker container‚Äôs working directory? ‚úÖ\n",
    "- How do Lambda container images work?\n",
    "- How can I annotate a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b76617",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do copy a file to a Docker container?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_plus(query, n_results, course_name):\n",
    "    try:\n",
    "        # Construct an Elasticsearch query with:\n",
    "        # - Multi-match for searching across multiple fields with different weights\n",
    "        # - Using only question and text fields with question having a boost of 4\n",
    "        search_query = {\n",
    "            \"size\": n_results,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    # Multi-match searches across multiple fields\n",
    "                    \"must\": {\n",
    "                                \"multi_match\": {\n",
    "                                    \"query\": query,\n",
    "                                    \"fields\": [\"question^4\", \"text\"],\n",
    "                                    \"type\": \"best_fields\"\n",
    "                                }\n",
    "            },\n",
    "                    # Filter to only include specific course documents\n",
    "                    # Filters don't affect relevance scoring, they just limit the result set\n",
    "                    \"filter\": {\n",
    "                        \"term\": {\n",
    "                            \"course\": course_name\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute search and get results\n",
    "        response = es_client_hw01.search(index=index_name, body=search_query)\n",
    "        \n",
    "        # Print the score for the top result\n",
    "        if response['hits']['hits']:\n",
    "            print(f\"Top result score: {response['hits']['hits'][0]['_score']}\")\n",
    "        \n",
    "        # Return the full response to examine all details\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching with Elasticsearch: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "900578c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result score: 73.38676\n",
      "Third hit score: 59.812744\n",
      "Third hit source: How do I copy files from a different folder into docker container‚Äôs working directory?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82822/98751384.py:29: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client_hw01.search(index=index_name, body=search_query)\n"
     ]
    }
   ],
   "source": [
    "elastic_response_2 =  elastic_search_plus(query, n_results=3, course_name=\"machine-learning-zoomcamp\")\n",
    "\n",
    "# the thrid result is the one we are looking for\n",
    "if elastic_response_2 and 'hits' in elastic_response_2:\n",
    "    top_hits = elastic_response_2['hits']['hits']\n",
    "    if len(top_hits) >= 3:\n",
    "        third_hit = top_hits[2]  # Get the third result\n",
    "        print(f\"Third hit score: {third_hit['_score']}\")\n",
    "        print(f\"Third hit source: {third_hit['_source']['question']}\")\n",
    "    else:\n",
    "        print(\"Less than 3 hits found\")\n",
    "else:\n",
    "    print(\"No valid response from Elasticsearch search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c09f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to Q4: {'_index': 'course-questions-hw01', '_id': 'E4JNmJcBC-rpw4cz6gjh', '_score': 59.812744, '_source': {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from a different folder into docker container‚Äôs working directory?', 'course': 'machine-learning-zoomcamp'}}\n"
     ]
    }
   ],
   "source": [
    "answer_q4 = elastic_response_2['hits']['hits'][2]\n",
    "\n",
    "print(f\"Answer to Q4: {answer_q4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df4e24",
   "metadata": {},
   "source": [
    "## Q5. Building a prompt\n",
    "Now we're ready to build a prompt to send to an LLM.\n",
    "\n",
    "Take the records returned from Elasticsearch in Q4 and use this template to build the context. Separate context entries by two linebreaks (\\n\\n)\n",
    "\n",
    "```bash\n",
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "Now use the context you just created along with the \"How do copy a file to a Docker container?\" question to construct a prompt using the template below:\n",
    "\n",
    "What's the length of the resulting prompt? (use the len function)\n",
    "\n",
    "- 946\n",
    "- 1446 ‚úÖ (closest one)\n",
    "- 1946\n",
    "- 2446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e937ee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'E4JNmJcBC-rpw4cz6gjh',\n",
      " '_index': 'course-questions-hw01',\n",
      " '_score': 59.812744,\n",
      " '_source': {'course': 'machine-learning-zoomcamp',\n",
      "             'question': 'How do I copy files from a different folder into '\n",
      "                         'docker container‚Äôs working directory?',\n",
      "             'section': '5. Deploying Machine Learning Models',\n",
      "             'text': 'You can copy files from your local machine into a Docker '\n",
      "                     \"container using the docker cp command. Here's how to do \"\n",
      "                     'it:\\n'\n",
      "                     'In the Dockerfile, you can provide the folder containing '\n",
      "                     'the files that you want to copy over. The basic syntax '\n",
      "                     'is as follows:\\n'\n",
      "                     'COPY [\"src/predict.py\", \"models/xgb_model.bin\", '\n",
      "                     '\"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(answer_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eed14c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    \"\"\"\n",
    "    Build a prompt for the LLM based on the query and search results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        search_results (list): List of search result dictionaries from Elasticsearch.\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted prompt for the LLM.\n",
    "    \"\"\"\n",
    "    # Template for context entries\n",
    "    context_template = \"\"\"\n",
    "    Q: {question}\n",
    "    A: {text}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Build context from search results\n",
    "    context_text = \"\"\n",
    "    for hit in search_results['hits']['hits']:\n",
    "        source = hit[\"_source\"]\n",
    "        context_entry = context_template.format(\n",
    "            question=source[\"question\"],\n",
    "            text=source[\"text\"]\n",
    "        )\n",
    "        context_text += context_entry + \"\\n\\n\"\n",
    "\n",
    "    # Template for overall prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Create final prompt\n",
    "    final_prompt = prompt_template.format(\n",
    "        question=query,\n",
    "        context=context_text\n",
    "    )\n",
    "    \n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8990cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the prompt: 1478\n",
      "\n",
      "Final Prompt:\n",
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "    QUESTION: How do I copy a file to a Docker container?\n",
      "\n",
      "    CONTEXT:\n",
      "    Q: How do I debug a docker container?\n",
      "    A: Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\n",
      "docker run -it --entrypoint bash <image>\n",
      "If the container is already running, execute a command in the specific container:\n",
      "docker ps (find the container-id)\n",
      "docker exec -it <container-id> bash\n",
      "(Marcos MJD)\n",
      "\n",
      "Q: How do I copy files from my local machine to docker container?\n",
      "    A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: How do I copy files from a different folder into docker container‚Äôs working directory?\n",
      "    A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_prompt = build_prompt(query, elastic_response_2)\n",
    "\n",
    "# Print the length of the resulting prompt\n",
    "print(f\"Length of the prompt: {len(final_prompt)}\")\n",
    "\n",
    "# Display the prompt\n",
    "print(\"\\nFinal Prompt:\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ef4aa",
   "metadata": {},
   "source": [
    "## Q6. Tokens\n",
    "When we use the OpenAI Platform, we're charged by the number of tokens we send in our prompt and receive in the response.\n",
    "\n",
    "The OpenAI python package uses tiktoken for tokenization:\n",
    "\n",
    "```bash\n",
    "    uv pip install tiktoken\n",
    "```\n",
    "\n",
    "Let's calculate the number of tokens in our query:\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "Use the encode function. How many tokens does our prompt have?\n",
    "\n",
    "- 120\n",
    "- 220\n",
    "- 320 ‚úÖ (closest one)\n",
    "- 420\n",
    "\n",
    "Note: to decode back a token into a word, you can use the decode_single_token_bytes function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8ed02d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 329\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def count_tokens(prompt):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given prompt using the specified encoding.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to count tokens for.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the prompt.\n",
    "    \"\"\"\n",
    "    return len(encoding.encode(prompt))\n",
    "\n",
    "token_count = count_tokens(final_prompt)\n",
    "print(f\"Number of tokens in the prompt: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a6449b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded Prompt:\n",
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "    QUESTION: How do I copy a file to a Docker container?\n",
      "\n",
      "    CONTEXT:\n",
      "    Q: How do I debug a docker container?\n",
      "    A: Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\n",
      "docker run -it --entrypoint bash <image>\n",
      "If the container is already running, execute a command in the specific container:\n",
      "docker ps (find the container-id)\n",
      "docker exec -it <container-id> bash\n",
      "(Marcos MJD)\n",
      "\n",
      "Q: How do I copy files from my local machine to docker container?\n",
      "    A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: How do I copy files from a different folder into docker container‚Äôs working directory?\n",
      "    A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decode the prompt to see how it looks\n",
    "decoded_prompt = encoding.decode(encoding.encode(final_prompt))\n",
    "print(\"\\nDecoded Prompt:\")\n",
    "print(decoded_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36465cbb",
   "metadata": {},
   "source": [
    "## Bonus: generating the answer (ungraded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0aaf1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for OpenAI integration\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (contains API keys)\n",
    "env_path = Path('../') / '.env'\n",
    "\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: .env file not found, make sure to set OPENAI_API_KEY manually\")\n",
    "\n",
    "# Access the API key\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eccb1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LLM function for generating responses\n",
    "def llm(prompt):\n",
    "    \"\"\"\n",
    "    Generate a response using OpenAI's LLM based on the provided prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The full prompt including query and context\n",
    "        \n",
    "    Returns:\n",
    "        string: Generated answer from the LLM\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',  # Using OpenAI's most capable model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0  # Lower temperature for more factual responses\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b984ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG pipeline implementation\n",
    "def rag(query):\n",
    "    \"\"\"\n",
    "    Implements the full Retrieval-Augmented Generation pipeline.\n",
    "    \n",
    "    The pipeline consists of three main steps:\n",
    "    1. RETRIEVAL: Find relevant documents using vector search\n",
    "    2. AUGMENTATION: Build a prompt that includes retrieved context\n",
    "    3. GENERATION: Generate an answer using the LLM with context\n",
    "    \n",
    "    In the RETRIEVAL step, the system performs a vector search to find documents\n",
    "    that are relevant to the user's query. This is done using the elastic_search\n",
    "    function, which interfaces with a vector database to fetch top-k documents.\n",
    "    \n",
    "    The AUGMENTATION step involves constructing a prompt for the language model.\n",
    "    The prompt is built by the build_prompt function, which takes the user's query\n",
    "    and the retrieved documents as input and creates a context-rich prompt.\n",
    "    \n",
    "    Finally, in the GENERATION step, the language model (LLM) generates an answer\n",
    "    to the query. The LLM is prompted with the contextually augmented prompt from\n",
    "    the previous step, enabling it to produce a well-informed answer.\n",
    "    \n",
    "    This pipeline effectively combines information retrieval and natural language\n",
    "    processing to provide accurate and contextually relevant answers to user queries.\n",
    "    It leverages the strengths of vector databases for retrieval and LLMs for\n",
    "    natural language understanding and generation.\n",
    "    \n",
    "    Args:\n",
    "        query: User's natural language question\n",
    "        \n",
    "    Returns:\n",
    "        string: Generated answer based on retrieved context\n",
    "    \"\"\"\n",
    "    # Step 1: RETRIEVAL - Get relevant documents using MinSearch\n",
    "    search_results = elastic_search(query)\n",
    "    \n",
    "    # Step 2: AUGMENTATION - Build prompt with retrieved context\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    \n",
    "    # Step 3: GENERATION - Generate answer using LLM\n",
    "    answer = llm(prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b3110110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82822/2332176584.py:17: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es_client_hw01.search(index=index_name, body=search_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result score: 84.050095\n",
      "\n",
      "RAG Answer: To copy a file to a Docker container, you can use the `docker cp` command. The basic syntax is as follows:\n",
      "\n",
      "```\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "```\n",
      "\n",
      "This command allows you to copy a file or directory from your local machine into a running Docker container.\n"
     ]
    }
   ],
   "source": [
    "rag_query = \"How do I copy a file to a Docker container?\"\n",
    "\n",
    "rag_answer = rag(rag_query)\n",
    "print(f\"\\nRAG Answer: {rag_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6962e8c",
   "metadata": {},
   "source": [
    "## Bonus: calculating the costs (ungraded)\n",
    "Suppose that on average per request we send 150 tokens and receive back 250 tokens.\n",
    "\n",
    "How much will it cost to run 1000 requests?\n",
    "\n",
    "You can see the prices here\n",
    "\n",
    "- [openai_pricing](https://platform.openai.com/docs/pricing)\n",
    "\n",
    "- Input: $0.15 / 1M tokens\n",
    "- Output: $0.60 / 1M tokens\n",
    "\n",
    "You can redo the calculations with the values you got in Q6 and Q7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "72c072fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost for RAG query: $0.000203\n"
     ]
    }
   ],
   "source": [
    "# how much does it cost to run this?\n",
    "# Assuming we use the gpt-4o-mini model, which costs $0.0001 per 1K tokens\n",
    "# Let's estimate the cost based on the prompt and response length\n",
    "def estimate_cost(prompt, response, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Estimate the cost of running the LLM based on prompt and response length.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt sent to the LLM.\n",
    "        response (str): The generated response from the LLM.\n",
    "        model (str): The model used for generation.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated cost in USD.\n",
    "    \"\"\"\n",
    "    # Token costs for gpt-4o-mini\n",
    "    token_cost_per_1k = 0.00015  # $0.00015 per 1000 tokens\n",
    "\n",
    "    # Count tokens in prompt and response\n",
    "    total_tokens = count_tokens(prompt) + count_tokens(response)\n",
    "\n",
    "    # Calculate cost\n",
    "    cost = (total_tokens / 1000) * token_cost_per_1k\n",
    "    return cost\n",
    "# Estimate the cost for the RAG query\n",
    "rag_cost = estimate_cost(final_prompt, rag_answer)\n",
    "print(f\"Estimated cost for RAG query: ${rag_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c859300",
   "metadata": {},
   "source": [
    "# Summary and Review of Homework 1\n",
    "\n",
    "In this homework assignment, we implemented a complete Retrieval-Augmented Generation (RAG) pipeline using Elasticsearch and OpenAI LLMs. Here's what we accomplished:\n",
    "\n",
    "## Environment Setup\n",
    "1. We set up and verified an Elasticsearch 8.12.2 instance running locally\n",
    "2. We created a Python environment with necessary libraries (elasticsearch-py, tiktoken, openai)\n",
    "\n",
    "## Data Preparation and Indexing\n",
    "1. We fetched JSON data containing questions and answers from DataTalksClub's GitHub repository\n",
    "2. We processed the data to include course information in each document\n",
    "3. We configured an Elasticsearch index with proper mappings:\n",
    "   - Text fields (question, text, section) for full-text searching\n",
    "   - Keyword field (course) for exact filtering\n",
    "\n",
    "## Search Functionality\n",
    "1. We implemented basic search using Elasticsearch's multi_match query\n",
    "2. We applied field boosting to prioritize matches in question fields (boost=4)\n",
    "3. We implemented filtered search to target specific courses\n",
    "4. We explored and understood Elasticsearch relevance scoring\n",
    "\n",
    "## RAG Implementation Components\n",
    "1. **Retrieval**: We used Elasticsearch to find relevant documents based on user queries\n",
    "2. **Augmentation**: We built a prompt template that incorporated retrieved context\n",
    "3. **Generation**: We connected to OpenAI's API to generate responses based on context\n",
    "\n",
    "## Analysis and Optimization\n",
    "1. We calculated token counts in our prompts using tiktoken\n",
    "2. We estimated costs based on token usage\n",
    "3. We explored the trade-offs between token count and retrieved context quality\n",
    "\n",
    "This assignment demonstrated how to build a practical RAG system that outperforms standard LLM responses by incorporating domain-specific knowledge from a custom knowledge base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
